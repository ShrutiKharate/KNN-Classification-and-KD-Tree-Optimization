{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCoMIYx6jUOW",
        "outputId": "cc8bf8c7-4546-4c88-d054-3fe28c6278ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'videogamesales' dataset.\n",
            "Path to dataset files: /kaggle/input/videogamesales\n",
            "--- Data Preparation Complete ---\n",
            "Features shape: (16327, 625)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, KDTree\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import time\n",
        "\n",
        "# --- Dataset Access (Only Download Code) ---\n",
        "path = kagglehub.dataset_download(\"gregorut/videogamesales\")\n",
        "print(f\"Path to dataset files: {path}\")\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the file is named 'vgsales.csv' within the downloaded path\n",
        "df = pd.read_csv(f\"{path}/vgsales.csv\")\n",
        "\n",
        "# --- Data Preparation (Guidelines) ---\n",
        "\n",
        "# 1. Drop rows with missing Year, Genre, Platform, or Global_Sales\n",
        "cols_to_check = ['Year', 'Genre', 'Platform', 'Global_Sales']\n",
        "df_clean = df.dropna(subset=cols_to_check).copy()\n",
        "\n",
        "# 2. Convert Year to integer and optionally clip (1980-2020)\n",
        "df_clean['Year'] = pd.to_numeric(df_clean['Year'], errors='coerce').astype('Int64')\n",
        "df_clean['Year'] = df_clean['Year'].clip(lower=1980, upper=2020)\n",
        "\n",
        "# 3. Define features (X) and targets (y)\n",
        "# Drop Name and Rank as they are usually not predictive features\n",
        "features = ['Year', 'Genre', 'Platform', 'Publisher', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
        "X = df_clean[features]\n",
        "\n",
        "# Global_Sales is the sum of regional sales, so we'll use regional sales as features\n",
        "# to avoid leakage in the regressor, as specified in the objective.\n",
        "# For the classifier, we use the original feature set (Year, Genre, Platform, etc.)\n",
        "\n",
        "# Separate data for Classifier (Genre prediction) and Regressor (Global_Sales prediction)\n",
        "# Classifier Target\n",
        "y_genre = df_clean['Genre']\n",
        "# Regressor Target\n",
        "y_sales = df_clean['Global_Sales']\n",
        "\n",
        "# --- Preprocessing Pipeline Setup ---\n",
        "\n",
        "# Define numeric and categorical features\n",
        "numeric_features = ['Year', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
        "# Platform, Publisher and Genre for one-hot encoding\n",
        "categorical_features = ['Platform', 'Publisher', 'Genre']\n",
        "\n",
        "# Create preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # 4. Use one-hot encoding for categorical features\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
        "        # 5. Scale numeric features\n",
        "        ('scaler', StandardScaler(), numeric_features)\n",
        "    ],\n",
        "    # Pass through other columns (not needed here since all are in one of the lists)\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Apply preprocessing to the full feature set\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "# Get feature names after one-hot encoding for analysis if needed\n",
        "feature_names = list(preprocessor.named_transformers_['onehot'].get_feature_names_out(categorical_features)) + numeric_features\n",
        "\n",
        "# Convert to DataFrame for easier inspection (optional)\n",
        "X_processed_df = pd.DataFrame(X_processed.toarray(), columns=feature_names, index=df_clean.index)\n",
        "print(\"--- Data Preparation Complete ---\")\n",
        "print(f\"Features shape: {X_processed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1) Build and evaluate a KNN classifier for Genre\n",
        "print(\"\\n--- KNN Classifier for Genre ---\")\n",
        "\n",
        "# Split data (using the cleaned data and Genre target)\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "    X_processed, y_genre, test_size=0.3, random_state=42, stratify=y_genre\n",
        ")\n",
        "\n",
        "# Define hyperparameters for initial model (k=5, metric=Euclidean)\n",
        "k_cls = 5\n",
        "metric_cls = 'euclidean'\n",
        "\n",
        "# Build and train the KNN Classifier model\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=k_cls, metric=metric_cls)\n",
        "knn_classifier.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_cls = knn_classifier.predict(X_test_cls)\n",
        "accuracy_cls = accuracy_score(y_test_cls, y_pred_cls)\n",
        "\n",
        "print(f\"Classifier (k={k_cls}, metric='{metric_cls}') Accuracy: **{accuracy_cls:.4f}**\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC1bED7blCdd",
        "outputId": "abe0104f-bf2b-4052-8902-5a5b9b727f3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- KNN Classifier for Genre ---\n",
            "Classifier (k=5, metric='euclidean') Accuracy: **0.9588**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2) Build and evaluate a KNN regressor for Global_Sales (avoiding leakage)\n",
        "print(\"\\n--- KNN Regressor for Global_Sales (Leakage Avoidance) ---\")\n",
        "\n",
        "# --- Revised Preprocessing for Regressor to Avoid Leakage ---\n",
        "# Features for Regressor: Only Year, Platform, Genre, Publisher (No regional sales)\n",
        "# We need to re-run the preprocessor without regional sales columns for X_regressor\n",
        "regressor_features = ['Year', 'Genre', 'Platform', 'Publisher']\n",
        "X_regressor_raw = df_clean[regressor_features]\n",
        "\n",
        "regressor_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), ['Platform', 'Publisher', 'Genre']),\n",
        "        ('scaler', StandardScaler(), ['Year'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_regressor = regressor_preprocessor.fit_transform(X_regressor_raw)\n",
        "\n",
        "# Split data (using the leakage-avoiding features and Global_Sales target)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_regressor, y_sales, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define hyperparameters for initial model (k=5, metric=Manhattan)\n",
        "k_reg = 5\n",
        "metric_reg = 'manhattan' # L1 norm often good for regression\n",
        "\n",
        "# Build and train the KNN Regressor model\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=k_reg, metric=metric_reg)\n",
        "knn_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_reg = knn_regressor.predict(X_test_reg)\n",
        "mse_reg = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "rmse_reg = np.sqrt(mse_reg)\n",
        "\n",
        "print(f\"Regressor (k={k_reg}, metric='{metric_reg}') RMSE: **{rmse_reg:.4f}** (in millions of dollars)\")\n",
        "print(\"> Lower RMSE is better. Note: This result is based on non-sales features to avoid leakage.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv2VAWL0lLpV",
        "outputId": "388096d1-aa5c-4a70-9707-4f76cde1e25b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- KNN Regressor for Global_Sales (Leakage Avoidance) ---\n",
            "Regressor (k=5, metric='manhattan') RMSE: **1.7843** (in millions of dollars)\n",
            "> Lower RMSE is better. Note: This result is based on non-sales features to avoid leakage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3) Construct a KD-tree\n",
        "print(\"\\n--- KD-Tree Construction and Nearest Neighbor Query ---\")\n",
        "\n",
        "# KD-Trees work best on data with fewer dimensions and Euclidean distance.\n",
        "# We'll use the classifier's scaled feature set (X_processed).\n",
        "# The KDTree class from sklearn.neighbors can be used directly.\n",
        "kdtree = KDTree(X_processed.toarray())\n",
        "print(\"KD-Tree built successfully on the full preprocessed dataset.\")\n",
        "\n",
        "# Example query: Find the 3 nearest neighbors to the first data point\n",
        "query_point = X_processed[0].toarray().reshape(1, -1)\n",
        "distances, indices = kdtree.query(query_point, k=3)\n",
        "\n",
        "print(f\"Query point index: {df_clean.index[0]}\")\n",
        "print(f\"Nearest neighbor indices: {df_clean.index[indices[0]]}\")\n",
        "print(f\"Distances: {distances[0]}\")\n",
        "\n",
        "\n",
        "## 4) Compare neighbor search backends (kd_tree, ball_tree, and brute)\n",
        "print(\"\\n--- Backend Comparison (Runtime) ---\")\n",
        "\n",
        "k_search = 10\n",
        "n_queries = 100 # Number of queries to average the search time\n",
        "\n",
        "# Generate a small set of random queries from the test set\n",
        "np.random.seed(42)\n",
        "query_indices = np.random.choice(X_test_cls.shape[0], n_queries, replace=False)\n",
        "queries = X_test_cls[query_indices]\n",
        "\n",
        "backends = ['brute', 'kd_tree', 'ball_tree']\n",
        "runtimes = {}\n",
        "\n",
        "for backend in backends:\n",
        "    # Use the classifier setup, but enforce the algorithm\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k_search, algorithm=backend)\n",
        "    knn_model.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for q in queries:\n",
        "        knn_model.kneighbors(q.reshape(1, -1))\n",
        "    end_time = time.time()\n",
        "\n",
        "    avg_time = (end_time - start_time) / n_queries\n",
        "    runtimes[backend] = avg_time\n",
        "\n",
        "print(pd.DataFrame({'Avg Query Time (s)': runtimes}).T)\n",
        "\n",
        "# Analysis:\n",
        "print(\"\\n**Analysis of Backend Runtimes:**\")\n",
        "if runtimes['brute'] == min(runtimes.values()):\n",
        "    print(\"Brute force was fastest. This often happens on datasets with **high dimensionality** or when the total number of data points is **small/medium**.\")\n",
        "elif runtimes['kd_tree'] == min(runtimes.values()):\n",
        "    print(\"KD-Tree was fastest. This usually indicates the data is well-suited for partitioning (i.e., **low-to-moderate dimensionality**).\")\n",
        "elif runtimes['ball_tree'] == min(runtimes.values()):\n",
        "    print(\"Ball-Tree was fastest. This is common with **high-dimensional data** or when using **non-Euclidean metrics**.\")\n",
        "\n",
        "\n",
        "## 5) Provide a concise analysis of hyperparameters (k, metric p in {1,2}), feature scaling, and dimensionality effects.\n",
        "print(\"\\n--- 5) Concise Analysis ---\")\n",
        "\n",
        "print(\"### Hyperparameters (k, metric p)\")\n",
        "print(\"* **k (Number of Neighbors):** Increasing *k* generally makes the model **smoother** and less sensitive to noise, reducing variance but potentially increasing bias (underfitting). A smaller *k* is more sensitive to local data, resulting in higher variance.\")\n",
        "print(\"* **Metric p $\\in \\{1, 2\\}$:**\")\n",
        "print(\"    * **$p=1$ (Manhattan Distance):** Measures distance by the sum of absolute differences (L1 norm). It's more **robust to outliers** and often preferred in high-dimensional spaces or for regression.\")\n",
        "print(\"    * **$p=2$ (Euclidean Distance):** Measures the shortest straight-line distance (L2 norm). It's the standard for spatial data and is the default for many KNN implementations.\")\n",
        "\n",
        "print(\"\\n### Feature Scaling\")\n",
        "print(\"* **Effect:** Scaling is **critical** for KNN. Since KNN relies on distance calculation (like Euclidean or Manhattan), features with a larger magnitude (e.g., Global_Sales) will **dominate** the distance function, regardless of their actual importance. `StandardScaler` (Z-score normalization) ensures all features contribute equally to the distance calculation.\")\n",
        "\n",
        "print(\"\\n### Dimensionality Effects (The Curse of Dimensionality)\")\n",
        "print(\"* **Effect:** In high-dimensional spaces (which the one-hot encoding creates), the distance between any two points tends to become **almost equal**. This means that the concept of 'nearest' neighbor loses its meaning.\")\n",
        "print(\"* **Consequence:** Algorithms like **KD-Tree** become less efficient, often defaulting to performance close to **Brute Force** search. **Ball-Tree** is generally more robust to higher dimensions. For effective KNN in high dimensions, dimensionality reduction (like PCA) may be necessary.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNyiDB5elML1",
        "outputId": "43fa289a-e869-4bcd-c460-92b0217f4e47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:63: SyntaxWarning: invalid escape sequence '\\i'\n",
            "<>:63: SyntaxWarning: invalid escape sequence '\\i'\n",
            "/tmp/ipython-input-3196360621.py:63: SyntaxWarning: invalid escape sequence '\\i'\n",
            "  print(\"* **Metric p $\\in \\{1, 2\\}$:**\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- KD-Tree Construction and Nearest Neighbor Query ---\n",
            "KD-Tree built successfully on the full preprocessed dataset.\n",
            "Query point index: 0\n",
            "Nearest neighbor indices: Index([0, 2, 3], dtype='int64')\n",
            "Distances: [ 0.         52.13631851 55.46657722]\n",
            "\n",
            "--- Backend Comparison (Runtime) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_base.py:598: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_base.py:598: UserWarning: cannot use tree with sparse input: using brute force\n",
            "  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       brute  kd_tree  ball_tree\n",
            "Avg Query Time (s)  0.001293  0.00185   0.001808\n",
            "\n",
            "**Analysis of Backend Runtimes:**\n",
            "Brute force was fastest. This often happens on datasets with **high dimensionality** or when the total number of data points is **small/medium**.\n",
            "\n",
            "--- 5) Concise Analysis ---\n",
            "### Hyperparameters (k, metric p)\n",
            "* **k (Number of Neighbors):** Increasing *k* generally makes the model **smoother** and less sensitive to noise, reducing variance but potentially increasing bias (underfitting). A smaller *k* is more sensitive to local data, resulting in higher variance.\n",
            "* **Metric p $\\in \\{1, 2\\}$:**\n",
            "    * **$p=1$ (Manhattan Distance):** Measures distance by the sum of absolute differences (L1 norm). It's more **robust to outliers** and often preferred in high-dimensional spaces or for regression.\n",
            "    * **$p=2$ (Euclidean Distance):** Measures the shortest straight-line distance (L2 norm). It's the standard for spatial data and is the default for many KNN implementations.\n",
            "\n",
            "### Feature Scaling\n",
            "* **Effect:** Scaling is **critical** for KNN. Since KNN relies on distance calculation (like Euclidean or Manhattan), features with a larger magnitude (e.g., Global_Sales) will **dominate** the distance function, regardless of their actual importance. `StandardScaler` (Z-score normalization) ensures all features contribute equally to the distance calculation.\n",
            "\n",
            "### Dimensionality Effects (The Curse of Dimensionality)\n",
            "* **Effect:** In high-dimensional spaces (which the one-hot encoding creates), the distance between any two points tends to become **almost equal**. This means that the concept of 'nearest' neighbor loses its meaning.\n",
            "* **Consequence:** Algorithms like **KD-Tree** become less efficient, often defaulting to performance close to **Brute Force** search. **Ball-Tree** is generally more robust to higher dimensions. For effective KNN in high dimensions, dimensionality reduction (like PCA) may be necessary.\n"
          ]
        }
      ]
    }
  ]
}